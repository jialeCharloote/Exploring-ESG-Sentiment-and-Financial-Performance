{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jialeCharloote/Exploring-ESG-Sentiment-and-Financial-Performance/blob/main/greenwashing%E2%80%94%E2%80%94Charlotte.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4kTVbXbaEOI"
      },
      "source": [
        "# Greenwashing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr4LxQ_NaEOJ"
      },
      "source": [
        "### Definition: Selective Disclosure\n",
        "\n",
        "Retain the disclosure of negative information related to the company’s environmental/social performance and expose positive information regarding its environmental/social performance.\n",
        "\n",
        "### Signs of Greenwashing (possible $X$)\n",
        "\n",
        "**levaraging self-disclosure & company commitment**\n",
        "\n",
        "1. diversities: “The second possibility is that divestitures of pollutive assets respond to external environmental pressures by transferring ownership from firms that face stronger pressures to firms that face weaker pressures (or are better at addressing those pressures)” ([Duchin et al., 2022, p. 3])\n",
        "\n",
        "2. using vague and ambitious words without pratical target and action: specific target vs vague descriptive words  （Policies vs Target）\n",
        "    \n",
        "3. no proof: an environmental claim that cannot be substantiated by easily accessible supporting information or by a reliable third-party certification\n",
        "\n",
        "### Identify Greenwashing (possible $Y$)\n",
        "\n",
        "**need more objective measure (eg: total polution/ violation & misconduct / employee gender diversity etc)**\n",
        "\n",
        "1. invloved in greenwashing controversies (related news comprise key words) eg: The list of keywords consists of ‘litigation’, ‘lawsuit’, ‘fine(d)’, ‘sue(d)’, ‘attorney’, ‘judge’, ‘lawyers’, ‘barrister’, ‘trial’, ‘court’, ‘legal’, ‘prosecuted/tion’\n",
        "\n",
        "2. companies that violate regulations/laws (news/twitter/regulation)\n",
        "\n",
        "\n",
        "\n",
        "### Related literature\n",
        "\n",
        "1. Greenwash: Corporate Environmental Disclosure under Threat of Audit [link:https://doi-org.proxy.uchicago.edu/10.1111/j.1530-9134.2010.00282.x]\n",
        "\n",
        "2. Diversity Washing (Baker et al., 2022):\n",
        "\n",
        "* Measures of diversity washing based on the intra-year distance between the **amount of DEI commitment discussion** and **actual diversity**.\n",
        "\n",
        "* calculate these deviations as the difference between a firm’s within-year DEIcommitment disclosure percentile and its diversity percentile.\n",
        "\n",
        "* Construct a binary variable that equals 1 if a firm’s disclosure percentile is higher than its diversity percentile, and 0 otherwise, and label the resulting variable Diversity Washer.\n",
        "\n",
        "$$\n",
        "Diversity\\_Washer = \\begin{cases}\n",
        "      1 & \\text{if } DEI\\_commitment\\_disclosure\\_percentile > diversity\\_percentile \\\\\n",
        "      0 & \\text{otherwise}\n",
        "   \\end{cases}\n",
        "$$\n",
        "\n",
        "3. Sustainability or Greenwashing: Evidence from the Asset Market for Industrial Pollution (Duchin et al., 2022)\n",
        "\n",
        "    Possion DiD regression:\n",
        "\n",
        "    $Y_{i,t} = \\beta Divested_i \\times Post_{i,t} + \\alpha_i + \\tau_t + \\epsilon_{i,t} \\quad (1)$\n",
        "\n",
        "    Y, include total pollution, pollution intensity, and pollution abatement activities such as source reduction and the percentage of waste being recycled, recovered, or treated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPCpfUEkaEOJ"
      },
      "source": [
        "### Investigating Greenwashing: An Analysis Snapshot\n",
        "\n",
        "\n",
        "**Motivation:** In the context of diversity washing, firms might also use less precise discussions to avoid litigation risk (e.g., Skinner, 1994).\n",
        "\n",
        "**Vagueness:** Follow Baker's method(2022), the terms they identify as vague words are: believe, can, commonly, could, help, leading, like, many, may, maybe, might, often, possibly, probably, rarely, seem, some, up to, virtually, and widely.\n",
        "\n",
        "**Strategy:** In the following code, I am trying to explore the rlationship between vague and ambiguous ESG discussions in corporate annual report and corporate misconduct behavior by levaraging 830 public firm 2022 10-k annual reports.\n",
        "\n",
        "However, after referring to related literature and searching related datasets online, I didn't find a comprehensive dataset regarding firms' ESG-related misconduct behavior in 2022, so I cannot get a good Y in my regression analysis. (Datasources like Goodjobsfirst's Violation Tracker data do comprehend firm misconduct information; however, it requires searching information by each firm, which requires hand coding. Due to time limits, I didn't finish this task.)\n",
        "\n",
        "\n",
        "**Conclusions:** Therefore, unable to procure suitable Y (dependent variable) for regression analysis, I sought to explore the potential links between vagueness scores and key financial metrics such as Return on Equity (ROE), solvency ratio, asset scores, and Total ESG Sentiment scores. By running OLS regression below, I do find that firms with higher ROE have significantly lower vagueness scores, while firms with higher solvency ratios tend to have higher vagueness scores. There also exists a positive relationship between ESG sentiment and vagueness in ESG discussions, which potentially indicates that firms would use more vague and positive tone in their ESG discussions when they have worse financial performance and facing higher risks.\n",
        "\n",
        "**Limitatons**: I just offer indicative evidence showing firms' disclosure and may use \"greenwashing strategy\" to conceal their bad financial performance. However, due to data limitations, I believe the key to investigate whether firms exhibit greenwashing behavior is to using objective evidence to show whether those firms truly walk the talk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aeRdQq8aEOJ",
        "outputId": "923c54ae-f6c8-4902-9895-7d85e49e39cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/charlotte/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/charlotte/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#prep: import the following packages\n",
        "from collections import Counter\n",
        "import os\n",
        "import os.path\n",
        "import string\n",
        "import nltk\n",
        "import csv\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import MWETokenizer  #import tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokenizer = MWETokenizer()\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords  #import the list of stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer  #import stemmer module\n",
        "stemmer = SnowballStemmer('english')\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import regex as re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1tfPI6WaEOK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkPJsVh6aEOK"
      },
      "outputs": [],
      "source": [
        "regex_digits = re.compile(r'\\d+')\n",
        "regex_whitespace = re.compile(r'\\s+')\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def clean_tokenize(text):\n",
        "    \"\"\"\n",
        "    Cleans and tokenizes the input text. This includes removing numbers, certain punctuation marks,\n",
        "    converting to lowercase, and eliminating stopwords. The remaining words are then stemmed.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The text to be processed.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of stemmed tokens from the input text, excluding stopwords and punctuation.\n",
        "\n",
        "    \"\"\"\n",
        "    # Remove numbers and specific characters\n",
        "    text_cleaned = regex_digits.sub('', text)\n",
        "    text_cleaned = text_cleaned.replace('”', '').replace('“', '').replace('—', ' ')\n",
        "\n",
        "    # Remove punctuations and convert characters to lower case, then trim whitespace\n",
        "    text_cleaned = \"\".join([char.lower() for char in text_cleaned if char not in string.punctuation])\n",
        "    text_cleaned = regex_whitespace.sub(' ', text_cleaned).strip()\n",
        "\n",
        "    # Tokenize, remove stopwords, and stem\n",
        "    tokens = word_tokenize(text_cleaned)\n",
        "    filtered_tokens = [stemmer.stem(word) for word in tokens if word not in english_stopwords]\n",
        "\n",
        "    return filtered_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vAf50osaEOK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_vagueness(sentence, vague_words):\n",
        "    \"\"\"\n",
        "    Evaluate the vagueness of a sentence based on a set of predefined vague words.\n",
        "\n",
        "    Args:\n",
        "    - sentence (str): The sentence to evaluate.\n",
        "    - vague_words (list): A list of predefined vague words.\n",
        "\n",
        "    Returns:\n",
        "    - vagueness_score (int): A score indicating the vagueness level of the sentence,\n",
        "      based on the frequency of occurrence of vague words.\n",
        "    \"\"\"\n",
        "    # Tokenize the sentence into words\n",
        "    words = word_tokenize(sentence.lower())\n",
        "\n",
        "    # Count occurrences of vague words in the sentence\n",
        "    vague_word_count = sum(1 for word in words if word in vague_words)\n",
        "\n",
        "    # Calculate vagueness score as a percentage of total words in the sentence\n",
        "    total_words = len(words)\n",
        "    if total_words == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return (vague_word_count / total_words) * 100\n",
        "\n",
        "# Define the list of vague words\n",
        "vague_words = ['believe', 'can', 'commonly', 'could', 'help', 'leading', 'like', 'many', 'may',\n",
        "               'maybe', 'might', 'often', 'possibly', 'probably', 'rarely', 'seem', 'some',\n",
        "               'up to', 'virtually', 'widely']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtJv-hptaEOK"
      },
      "outputs": [],
      "source": [
        "def calculate_vagueness(text, vague_words):\n",
        "    \"\"\"\n",
        "    Calculate the vagueness score for a given text based on the frequency of occurrence of vague words.\n",
        "    \"\"\"\n",
        "    # Tokenize the text into words\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Count occurrences of vague words in the text\n",
        "    vague_word_count = sum(1 for word in tokens if word in vague_words)\n",
        "\n",
        "    # Calculate the total number of words in the text for normalization\n",
        "    total_words = len(tokens)\n",
        "\n",
        "    # Normalize the vagueness score\n",
        "    vagueness_score = vague_word_count / total_words if total_words > 0 else 0\n",
        "\n",
        "    return vagueness_score\n",
        "\n",
        "def append_vagueness_scores_to_df(data, vague_words):\n",
        "    \"\"\"\n",
        "    Appends normalized vagueness scores as a new column to the DataFrame.\n",
        "    \"\"\"\n",
        "    # Initialize column for vagueness scores\n",
        "    data['Vagueness Score'] = 0\n",
        "\n",
        "    # Calculate and append scores for each row\n",
        "    for index, row in data.iterrows():\n",
        "        vagueness_score = calculate_vagueness(row['Content'], vague_words)\n",
        "        data.at[index, 'Vagueness Score'] = vagueness_score\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndTIlugoaEOK"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = pd.read_csv(\"data_2022_cleaned.csv\", encoding=\"latin1\") ###Thhis dataset already contains firms financial performance, related esg sentiment score and extract related esg discussion in corporate annual reoort\n",
        "\n",
        "# Define the list of vague words\n",
        "vague_words = ['believe', 'can', 'commonly', 'could', 'help', 'leading', 'like', 'many', 'may',\n",
        "               'maybe', 'might', 'often', 'possibly', 'probably', 'rarely', 'seem', 'some',\n",
        "               'up to', 'virtually', 'widely']\n",
        "\n",
        "# Append vagueness scores to the DataFrame\n",
        "data = append_vagueness_scores_to_df(data, vague_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7CGYOnBaEOK",
        "outputId": "bfe76d11-c950-4616-d3fb-aba20e28b7d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of 0       0.009156\n",
              "2       0.012191\n",
              "4       0.011236\n",
              "6       0.006420\n",
              "8       0.007405\n",
              "          ...   \n",
              "1652    0.011661\n",
              "1654    0.011869\n",
              "1656    0.007255\n",
              "1658    0.011295\n",
              "1660    0.003279\n",
              "Name: Vagueness Score, Length: 830, dtype: float64>"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "data['Vagueness Score'].describe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_mEVIICaEOK",
        "outputId": "6808d445-46ea-4db4-c945-735e1942a13c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:        Vagueness Score   R-squared:                       0.047\n",
            "Model:                            OLS   Adj. R-squared:                  0.042\n",
            "Method:                 Least Squares   F-statistic:                     10.06\n",
            "Date:                Thu, 11 Apr 2024   Prob (F-statistic):           5.91e-08\n",
            "Time:                        22:41:26   Log-Likelihood:                 3586.0\n",
            "No. Observations:                 830   AIC:                            -7162.\n",
            "Df Residuals:                     825   BIC:                            -7138.\n",
            "Df Model:                           4                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=======================================================================================\n",
            "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
            "---------------------------------------------------------------------------------------\n",
            "const                   0.0062      0.000     13.891      0.000       0.005       0.007\n",
            "Total ESG Sentiment     0.0027      0.002      1.142      0.254      -0.002       0.007\n",
            "roe                  -1.66e-05   4.29e-06     -3.871      0.000    -2.5e-05   -8.18e-06\n",
            "solvency             2.071e-05   6.21e-06      3.335      0.001    8.52e-06    3.29e-05\n",
            "asset               -6.441e-12    2.8e-12     -2.304      0.021   -1.19e-11   -9.53e-13\n",
            "==============================================================================\n",
            "Omnibus:                       82.292   Durbin-Watson:                   1.874\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              107.756\n",
            "Skew:                           0.792   Prob(JB):                     3.99e-24\n",
            "Kurtosis:                       3.777   Cond. No.                     9.26e+08\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 9.26e+08. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n"
          ]
        }
      ],
      "source": [
        "# main regression\n",
        "X = data[[\"Total ESG Sentiment\",\"roe\", \"solvency\", \"asset\"]]\n",
        "y = data['Vagueness Score']\n",
        "\n",
        "# Add a constant to the independent variable for the intercept term\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Print the regression summary\n",
        "print(model.summary())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}